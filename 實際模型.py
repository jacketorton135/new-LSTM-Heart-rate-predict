import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split, StratifiedKFold
from tensorflow.keras.models import Sequential, save_model, load_model
from tensorflow.keras.layers import Dense, LSTM, Dropout, BatchNormalization, Bidirectional
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
import matplotlib.pyplot as plt
import seaborn as sns
import os
import joblib
from sklearn.metrics import classification_report, accuracy_score, roc_curve, auc, precision_recall_curve
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel
import warnings
warnings.filterwarnings('ignore')

# è¨­å®šéš¨æ©Ÿç¨®å­ä»¥ç¢ºä¿çµæœå¯é‡ç¾
np.random.seed(42)
tf.random.set_seed(42)

def load_and_preprocess_data(file_path, test_size=0.2):
    """
    åŠ è¼‰æ•¸æ“šã€é€²è¡Œç‰¹å¾µå·¥ç¨‹ä¸¦åˆ†å‰²ç‚ºè¨“ç·´é›†å’Œæ¸¬è©¦é›†
    
    åƒæ•¸ï¼š
    file_path: CSV æª”æ¡ˆçš„è·¯å¾‘
    test_size: æ¸¬è©¦é›†æ¯”ä¾‹ï¼Œé»˜èªç‚º 0.2
    
    è¿”å›ï¼š
    X_train, X_test: è¨“ç·´å’Œæ¸¬è©¦ç‰¹å¾µ
    y_train, y_test: è¨“ç·´å’Œæ¸¬è©¦æ¨™ç±¤
    feature_columns: ç‰¹å¾µæ¬„ä½åç¨±
    scaler: å·²æ“¬åˆçš„æ¨™æº–åŒ–å™¨ï¼Œç”¨æ–¼å°‡ä¾†çš„æ•¸æ“šè½‰æ›
    """
    # å¾ CSV æª”æ¡ˆè®€å–æ•¸æ“š
    dataframe = pd.read_csv(file_path)
    
    # æŸ¥çœ‹æ•¸æ“šä¸­æ˜¯å¦æœ‰ç¼ºå¤±å€¼ï¼Œä¸¦è™•ç†
    print("æª¢æŸ¥ç¼ºå¤±å€¼ï¼š")
    print(dataframe.isnull().sum())
    
    # è™•ç†å¯èƒ½çš„ç¼ºå¤±å€¼
    dataframe = dataframe.fillna(dataframe.median())
    
    # å®šç¾©ç”¨æ–¼é æ¸¬çš„ç‰¹å¾µåˆ—
    feature_columns = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 
                      'thalach', 'exang', 'oldpeak', 'slope', 'thal']
    
    # ç‰¹å¾µå·¥ç¨‹ï¼šæ·»åŠ æ–°ç‰¹å¾µ
    # å¹´é½¡èˆ‡å¿ƒç‡çš„æ¯”ç‡ï¼ˆç”Ÿç†æ„ç¾©ï¼šå¹´é½¡è¶Šå¤§ï¼Œæœ€å¤§å¿ƒç‡é€šå¸¸è¶Šä½ï¼‰
    dataframe['age_thalach_ratio'] = dataframe['age'] / (dataframe['thalach'] + 1)
    feature_columns.append('age_thalach_ratio')
    
    # è¡€å£“èˆ‡å¿ƒç‡çš„æ¯”ç‡ï¼ˆå¯èƒ½èˆ‡å¿ƒè‡Ÿè² è·æœ‰é—œï¼‰
    dataframe['bp_thalach_ratio'] = dataframe['trestbps'] / (dataframe['thalach'] + 1)
    feature_columns.append('bp_thalach_ratio')
    
    # è†½å›ºé†‡èˆ‡å¹´é½¡çš„æ¯”ç‡ï¼ˆå¹´é½¡æ¨™æº–åŒ–çš„è†½å›ºé†‡æ°´å¹³ï¼‰
    dataframe['chol_age_ratio'] = dataframe['chol'] / (dataframe['age'] + 1)
    feature_columns.append('chol_age_ratio')
    
    # åˆ‡åˆ†ç‰¹å¾µ (X) å’Œç›®æ¨™è®Šé‡ (y)
    X = dataframe[feature_columns].values
    y = dataframe['target'].values
    
    # ç‰¹å¾µé¸æ“‡ï¼šä½¿ç”¨éš¨æ©Ÿæ£®æ—æ‰¾å‡ºæœ€é‡è¦çš„ç‰¹å¾µ
    rf = RandomForestClassifier(n_estimators=100, random_state=42)
    rf.fit(X, y)
    
    # é¡¯ç¤ºç‰¹å¾µé‡è¦æ€§
    importance_df = pd.DataFrame({
        'Feature': feature_columns,
        'Importance': rf.feature_importances_
    }).sort_values('Importance', ascending=False)
    
    print("\nç‰¹å¾µé‡è¦æ€§ï¼š")
    print(importance_df)
    
    # é¸æ“‡å‰ 90% é‡è¦çš„ç‰¹å¾µ
    selector = SelectFromModel(rf, threshold="mean", prefit=True)
    X_selected = selector.transform(X)
    selected_indices = selector.get_support()
    selected_features = [feature_columns[i] for i in range(len(feature_columns)) if selected_indices[i]]
    
    print(f"\né¸æ“‡çš„ç‰¹å¾µ ({len(selected_features)}/{len(feature_columns)}):")
    print(selected_features)
    
    # åˆ†å‰²è¨“ç·´é›†å’Œæ¸¬è©¦é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X_selected, y, test_size=test_size, random_state=42, stratify=y
    )
    
    # ç‰¹å¾µæ¨™æº–åŒ–
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)
    
    return X_train, X_test, y_train, y_test, selected_features, scaler, selector

def create_LSTM_model(input_dimension, dropout_rate=0.5):
    """
    å‰µå»ºæ”¹é€²çš„ LSTM ç¥ç¶“ç¶²çµ¡æ¨¡å‹
    
    åƒæ•¸ï¼š
    input_dimension: è¼¸å…¥ç‰¹å¾µçš„ç¶­åº¦
    dropout_rate: Dropout å±¤çš„æ¯”ç‡ï¼Œç”¨æ–¼é˜²æ­¢éæ“¬åˆ
    
    è¿”å›ï¼š
    ç·¨è­¯éçš„ Keras æ¨¡å‹
    """
    model = Sequential([
        # é›™å‘ LSTMï¼Œå¯ä»¥å¾å…©å€‹æ–¹å‘å­¸ç¿’åºåˆ—æ¨¡å¼
        Bidirectional(LSTM(128, activation='tanh', return_sequences=True, 
                           kernel_regularizer=tf.keras.regularizers.l2(0.001)), 
                      input_shape=(input_dimension, 1)),
        BatchNormalization(),  # åŠ é€Ÿè¨“ç·´ä¸¦æä¾›è¼•å¾®çš„æ­£å‰‡åŒ–æ•ˆæœ
        Dropout(dropout_rate),  # é˜²æ­¢éæ“¬åˆ
        
        # ç¬¬äºŒå±¤ LSTM
        Bidirectional(LSTM(64, activation='tanh', return_sequences=False)),
        BatchNormalization(),
        Dropout(dropout_rate),
        
        # å…¨é€£æ¥å±¤
        Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        BatchNormalization(),
        Dropout(0.3),
        
        # è¼¸å‡ºå±¤ï¼ŒäºŒåˆ†é¡å•é¡Œ
        Dense(2, activation='softmax')
    ])
    
    # ç·¨è­¯æ¨¡å‹
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
        loss='categorical_crossentropy',
        metrics=['accuracy', tf.keras.metrics.AUC(), 
                tf.keras.metrics.Precision(), 
                tf.keras.metrics.Recall()]
    )
    
    return model

def train_with_cross_validation(X, y, input_dimension, n_splits=5):
    """
    ä½¿ç”¨äº¤å‰é©—è­‰è¨“ç·´æ¨¡å‹
    
    åƒæ•¸ï¼š
    X: ç‰¹å¾µæ•¸æ“š
    y: æ¨™ç±¤
    input_dimension: è¼¸å…¥ç‰¹å¾µçš„ç¶­åº¦
    n_splits: äº¤å‰é©—è­‰æ‹†åˆ†æ•¸
    
    è¿”å›ï¼š
    æœ€ä½³æ¨¡å‹
    è¨“ç·´æ­·å²
    """
    # æº–å‚™äº¤å‰é©—è­‰
    kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
    
    fold_accuracies = []
    fold_aucs = []
    best_accuracy = 0
    best_model = None
    best_history = None
    
    for fold, (train_idx, val_idx) in enumerate(kfold.split(X, y)):
        print(f"\nè¨“ç·´æŠ˜ç–Š {fold+1}/{n_splits}")
        
        # åˆ†å‰²æ•¸æ“š
        X_train_fold, X_val_fold = X[train_idx], X[val_idx]
        y_train_fold, y_val_fold = y[train_idx], y[val_idx]
        
        # é‡å¡‘æ•¸æ“šä»¥é©æ‡‰ LSTM çš„è¼¸å…¥æ ¼å¼
        X_train_reshaped = X_train_fold.reshape(-1, input_dimension, 1)
        X_val_reshaped = X_val_fold.reshape(-1, input_dimension, 1)
        
        # å°‡æ¨™ç±¤è½‰æ›ç‚º one-hot ç·¨ç¢¼
        y_train_cat = to_categorical(y_train_fold, num_classes=2)
        y_val_cat = to_categorical(y_val_fold, num_classes=2)
        
        # å‰µå»ºæ¨¡å‹
        model = create_LSTM_model(input_dimension)
        
        # å›èª¿å‡½æ•¸ - ä¿®å¾© .h5 å•é¡Œ
        callbacks = [
            # æå‰åœæ­¢ï¼Œé¿å…éæ“¬åˆ
            EarlyStopping(
                monitor='val_accuracy',
                patience=30,
                restore_best_weights=True
            ),
            # å‹•æ…‹èª¿æ•´å­¸ç¿’ç‡
            ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=10,
                min_lr=0.00001
            )
            # ç§»é™¤ ModelCheckpointï¼Œä»¥é¿å…æª”æ¡ˆæ ¼å¼éŒ¯èª¤
        ]
        
        # è¨“ç·´æ¨¡å‹
        history = model.fit(
            X_train_reshaped, y_train_cat,
            validation_data=(X_val_reshaped, y_val_cat),
            epochs=300,  # è¶³å¤ çš„è¨“ç·´è¼ªæ¬¡ï¼Œé…åˆ EarlyStopping
            batch_size=4,
            callbacks=callbacks,
            verbose=1
        )
        
        # è©•ä¼°æ¨¡å‹
        y_val_pred = model.predict(X_val_reshaped)
        val_accuracy = accuracy_score(y_val_fold, np.argmax(y_val_pred, axis=1))
        
        # è¨ˆç®— AUC
        y_val_true_one_hot = to_categorical(y_val_fold, num_classes=2)
        val_auc = roc_auc_score(y_val_true_one_hot, y_val_pred)
        
        print(f"æŠ˜ç–Š {fold+1} é©—è­‰æº–ç¢ºç‡: {val_accuracy:.4f}, AUC: {val_auc:.4f}")
        fold_accuracies.append(val_accuracy)
        fold_aucs.append(val_auc)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_accuracy > best_accuracy:
            best_accuracy = val_accuracy
            best_model = model
            best_history = history
    
    print(f"\näº¤å‰é©—è­‰å¹³å‡æº–ç¢ºç‡: {np.mean(fold_accuracies):.4f} Â± {np.std(fold_accuracies):.4f}")
    print(f"äº¤å‰é©—è­‰å¹³å‡ AUC: {np.mean(fold_aucs):.4f} Â± {np.std(fold_aucs):.4f}")
    
    return best_model, best_history

def train_final_model(X_train, y_train, X_test, y_test, input_dimension):
    """
    è¨“ç·´æœ€çµ‚æ¨¡å‹
    
    åƒæ•¸ï¼š
    X_train, y_train: è¨“ç·´æ•¸æ“šå’Œæ¨™ç±¤
    X_test, y_test: æ¸¬è©¦æ•¸æ“šå’Œæ¨™ç±¤
    input_dimension: è¼¸å…¥ç‰¹å¾µçš„ç¶­åº¦
    
    è¿”å›ï¼š
    è¨“ç·´å¥½çš„æ¨¡å‹
    æ¨¡å‹è¨“ç·´æ­·å²
    """
    # é‡å¡‘æ•¸æ“šä»¥é©æ‡‰ LSTM çš„è¼¸å…¥æ ¼å¼
    X_train_reshaped = X_train.reshape(-1, input_dimension, 1)
    X_test_reshaped = X_test.reshape(-1, input_dimension, 1)
    
    # å°‡æ¨™ç±¤è½‰æ›ç‚º one-hot ç·¨ç¢¼
    y_train_cat = to_categorical(y_train, num_classes=2)
    y_test_cat = to_categorical(y_test, num_classes=2)
    
    # å‰µå»ºæ¨¡å‹
    model = create_LSTM_model(input_dimension)
    
    # å›èª¿å‡½æ•¸ - ä¿®å¾© .h5 å•é¡Œ
    callbacks = [
        # æå‰åœæ­¢ï¼Œé¿å…éæ“¬åˆ
        EarlyStopping(
            monitor='val_accuracy',
            patience=30,
            restore_best_weights=True
        ),
        # å‹•æ…‹èª¿æ•´å­¸ç¿’ç‡
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=10,
            min_lr=0.00001
        )
        # ç§»é™¤ ModelCheckpointï¼Œä»¥é¿å…æª”æ¡ˆæ ¼å¼éŒ¯èª¤
    ]
    
    # è¨“ç·´æ¨¡å‹
    history = model.fit(
        X_train_reshaped, y_train_cat,
        validation_data=(X_test_reshaped, y_test_cat),
        epochs=300,
        batch_size=32,
        callbacks=callbacks,
        verbose=1
    )
    
    return model, history

def visualize_training_history(history):
    """
    è¦–è¦ºåŒ–è¨“ç·´æ­·å²
    
    åƒæ•¸ï¼š
    history: æ¨¡å‹è¨“ç·´æ­·å²
    """
    # ç¹ªè£½æº–ç¢ºç‡æ­·å²
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    
    # ç¹ªè£½æå¤±æ­·å²
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    
    plt.tight_layout()
    plt.show()

def evaluate_model(model, X_test, y_test):
    """
    è©•ä¼°æ¨¡å‹ä¸¦é¡¯ç¤ºçµæœ
    
    åƒæ•¸ï¼š
    model: è¨“ç·´å¥½çš„æ¨¡å‹
    X_test: æ¸¬è©¦æ•¸æ“š
    y_test: æ¸¬è©¦æ¨™ç±¤
    
    è¿”å›ï¼š
    æº–ç¢ºç‡ï¼ˆfloatï¼‰
    """
    # é‡å¡‘æ¸¬è©¦æ•¸æ“š
    X_test_reshaped = X_test.reshape(-1, X_test.shape[1], 1)
    
    # ç²å–é æ¸¬çµæœ
    y_pred_probs = model.predict(X_test_reshaped)
    y_pred = np.argmax(y_pred_probs, axis=1)
    
    # One-hot ç·¨ç¢¼çš„çœŸå¯¦æ¨™ç±¤ï¼Œç”¨æ–¼ ROC æ›²ç·š
    y_test_cat = to_categorical(y_test, num_classes=2)
    
    # è¨ˆç®—æº–ç¢ºç‡
    accuracy = accuracy_score(y_test, y_pred)
    print(f"\nâœ… æœ€çµ‚æ¨¡å‹æ¸¬è©¦é›†æº–ç¢ºç‡: {accuracy:.4f}\n")
    
    # æ··æ·†çŸ©é™£
    class_names = ['ç„¡å¿ƒè‡Ÿç—…', 'æœ‰å¿ƒè‡Ÿç—…']
    cm = tf.math.confusion_matrix(y_test, y_pred)
    
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names)
    plt.title('æœ€çµ‚æ¨¡å‹æ··æ·†çŸ©é™£')
    plt.xlabel('é æ¸¬æ¨™ç±¤')
    plt.ylabel('çœŸå¯¦æ¨™ç±¤')
    plt.tight_layout()
    plt.show()
    
    # ROC æ›²ç·š
    plt.figure(figsize=(8, 6))
    fpr, tpr, _ = roc_curve(y_test, y_pred_probs[:, 1])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'ROC æ›²ç·š (AUC = {roc_auc:.4f})')
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlabel('å½é™½æ€§ç‡ (False Positive Rate)')
    plt.ylabel('çœŸé™½æ€§ç‡ (True Positive Rate)')
    plt.title('ROC æ›²ç·š')
    plt.legend(loc='lower right')
    plt.tight_layout()
    plt.show()
    
    # PR æ›²ç·š
    plt.figure(figsize=(8, 6))
    precision, recall, _ = precision_recall_curve(y_test, y_pred_probs[:, 1])
    plt.plot(recall, precision, label='Precision-Recall æ›²ç·š')
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Precision-Recall æ›²ç·š')
    plt.legend(loc='lower left')
    plt.tight_layout()
    plt.show()
    
    # é¡¯ç¤ºåˆ†é¡å ±å‘Š
    print("\nğŸ“Š åˆ†é¡å ±å‘Š:")
    print(classification_report(y_test, y_pred, target_names=class_names))
    
    return accuracy, roc_auc

def save_deployment_model(model, scaler, selector, feature_names, model_dir='./model'):
    """
    ä¿å­˜æ¨¡å‹å’Œç›¸é—œè½‰æ›å™¨ç”¨æ–¼éƒ¨ç½²
    
    åƒæ•¸ï¼š
    model: è¨“ç·´å¥½çš„æ¨¡å‹
    scaler: æ¨™æº–åŒ–å™¨
    selector: ç‰¹å¾µé¸æ“‡å™¨
    feature_names: åŸå§‹ç‰¹å¾µåç¨±åˆ—è¡¨
    model_dir: ä¿å­˜æ¨¡å‹çš„ç›®éŒ„
    """
    # å‰µå»ºç›®éŒ„ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    os.makedirs(model_dir, exist_ok=True)
    
    # æª¢æŸ¥æ‚¨çš„ TensorFlow ç‰ˆæœ¬ï¼Œæ±ºå®šæ¨¡å‹ä¿å­˜æ–¹å¼
    if tf.__version__.startswith('2.'):
        # TF 2.x ç‰ˆæœ¬
        try:
            # é¦–å…ˆå˜—è©¦ .h5 æ ¼å¼
            save_model(model, os.path.join(model_dir, 'heart_disease_model.h5'))
        except:
            # å¦‚æœå¤±æ•—ï¼Œå˜—è©¦èˆŠç‰ˆçš„ SavedModel æ ¼å¼
            tf.saved_model.save(model, os.path.join(model_dir, 'heart_disease_model'))
            print("ä»¥ SavedModel æ ¼å¼ä¿å­˜æ¨¡å‹")
    else:
        # èˆŠç‰ˆæœ¬çš„ TF
        save_model(model, os.path.join(model_dir, 'heart_disease_model.h5'))
    
    # ä¿å­˜æ¨™æº–åŒ–å™¨
    joblib.dump(scaler, os.path.join(model_dir, 'scaler.pkl'))
    
    # ä¿å­˜ç‰¹å¾µé¸æ“‡å™¨
    joblib.dump(selector, os.path.join(model_dir, 'feature_selector.pkl'))
    
    # ä¿å­˜ç‰¹å¾µåç¨±åˆ—è¡¨
    with open(os.path.join(model_dir, 'feature_names.txt'), 'w') as f:
        f.write('\n'.join(feature_names))
    
    print(f"æ¨¡å‹å’Œè½‰æ›å™¨å·²ä¿å­˜è‡³ {model_dir}")

def predict_heart_disease(patient_data, model_dir='./model'):
    """
    ä½¿ç”¨å·²è¨“ç·´çš„æ¨¡å‹é€²è¡Œå¿ƒè‡Ÿç—…é æ¸¬
    
    åƒæ•¸ï¼š
    patient_data: æ‚£è€…æ•¸æ“šï¼Œå­—å…¸æ ¼å¼çš„ç‰¹å¾µ
    model_dir: æ¨¡å‹ç›®éŒ„
    
    è¿”å›ï¼š
    é æ¸¬çµæœå’Œæ¦‚ç‡
    """
    # æ ¹æ“šæ¨¡å‹ä¿å­˜æ ¼å¼æ±ºå®šè¼‰å…¥æ–¹å¼
    model_path = os.path.join(model_dir, 'heart_disease_model.h5')
    if os.path.exists(model_path):
        model = load_model(model_path)
    else:
        model_path = os.path.join(model_dir, 'heart_disease_model')
        if os.path.exists(model_path):
            model = tf.saved_model.load(model_path)
        else:
            raise FileNotFoundError("æ‰¾ä¸åˆ°å·²ä¿å­˜çš„æ¨¡å‹")
    
    scaler = joblib.load(os.path.join(model_dir, 'scaler.pkl'))
    selector = joblib.load(os.path.join(model_dir, 'feature_selector.pkl'))
    
    # è®€å–ç‰¹å¾µåç¨±
    with open(os.path.join(model_dir, 'feature_names.txt'), 'r') as f:
        feature_names = f.read().splitlines()
    
    # æ·»åŠ æ´¾ç”Ÿç‰¹å¾µ
    patient_data['age_thalach_ratio'] = patient_data['age'] / (patient_data['thalach'] + 1)
    patient_data['bp_thalach_ratio'] = patient_data['trestbps'] / (patient_data['thalach'] + 1)
    patient_data['chol_age_ratio'] = patient_data['chol'] / (patient_data['age'] + 1)
    
    # æº–å‚™ç‰¹å¾µï¼Œç¢ºä¿é †åºæ­£ç¢º
    input_features = []
    for feature in feature_names:
        if feature in patient_data:
            input_features.append(patient_data[feature])
    
    # è½‰æ›ç‚º numpy æ•¸çµ„
    patient_features = np.array([input_features])
    
    # ç‰¹å¾µé¸æ“‡
    patient_features_selected = selector.transform(patient_features)
    
    # æ¨™æº–åŒ–ç‰¹å¾µ
    patient_features_scaled = scaler.transform(patient_features_selected)
    
    # é‡å¡‘æ•¸æ“šä»¥é©æ‡‰ LSTM è¼¸å…¥æ ¼å¼
    patient_features_reshaped = patient_features_scaled.reshape(-1, patient_features_scaled.shape[1], 1)
    
    # é æ¸¬
    prediction_probs = model.predict(patient_features_reshaped)[0]
    prediction_class = np.argmax(prediction_probs)
    
    # çµæœ
    result = {
        'prediction': 'æœ‰å¿ƒè‡Ÿç—…' if prediction_class == 1 else 'ç„¡å¿ƒè‡Ÿç—…',
        'probability': float(prediction_probs[prediction_class]),
        'heart_disease_prob': float(prediction_probs[1])
    }
    
    return result

def log_roc_score(model, X_test, y_test, log_file='model_performance_log.txt'):
    """
    è¨˜éŒ„æ¨¡å‹çš„ ROC åˆ†æ•¸åˆ°æ—¥èªŒæ–‡ä»¶ï¼Œç”¨æ–¼è¿½è¹¤æ€§èƒ½æ”¹é€²
    
    åƒæ•¸ï¼š
    model: è¨“ç·´å¥½çš„æ¨¡å‹
    X_test: æ¸¬è©¦æ•¸æ“š
    y_test: æ¸¬è©¦æ¨™ç±¤
    log_file: æ—¥èªŒæ–‡ä»¶å
    """
    # é‡å¡‘æ¸¬è©¦æ•¸æ“š
    X_test_reshaped = X_test.reshape(-1, X_test.shape[1], 1)
    
    # ç²å–é æ¸¬çµæœ
    y_pred_probs = model.predict(X_test_reshaped)
    
    # One-hot ç·¨ç¢¼çš„çœŸå¯¦æ¨™ç±¤
    y_test_cat = to_categorical(y_test, num_classes=2)
    
    # è¨ˆç®— ROC AUC
    fpr, tpr, _ = roc_curve(y_test, y_pred_probs[:, 1])
    roc_auc = auc(fpr, tpr)
    
    # ç²å–ç•¶å‰æ™‚é–“
    from datetime import datetime
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # è¨˜éŒ„åˆ°æ–‡ä»¶
    with open(log_file, 'a') as f:
        f.write(f"{current_time}, AUC: {roc_auc:.4f}\n")
    
    return roc_auc

# ä¸»ç¨‹åº
def main():
    # è³‡æ–™æ–‡ä»¶è·¯å¾‘
    file_path = r"E:\å¿ƒè‡Ÿç—…\heart2.csv"  # è«‹æ›´æ”¹ç‚ºæ‚¨çš„å¯¦éš›è·¯å¾‘
    
    print("======= å¿ƒè‡Ÿç—…é æ¸¬æ¨¡å‹è¨“ç·´ (æ”¹é€²ç‰ˆ) =======")
    
    # åŠ è¼‰å’Œé è™•ç†æ•¸æ“š
    X_train, X_test, y_train, y_test, selected_features, scaler, selector = load_and_preprocess_data(file_path)
    
    print(f"\næœ€çµ‚é¸æ“‡çš„ç‰¹å¾µæ•¸é‡: {X_train.shape[1]}")
    
    # ä½¿ç”¨äº¤å‰é©—è­‰è¨“ç·´æ¨¡å‹
    print("\nä½¿ç”¨ 5 æŠ˜äº¤å‰é©—è­‰è¨“ç·´æ¨¡å‹...")
    cv_model, cv_history = train_with_cross_validation(
        np.concatenate((X_train, X_test)), 
        np.concatenate((y_train, y_test)), 
        X_train.shape[1], 
        n_splits=5
    )
    
    # åœ¨æœ€çµ‚æ•¸æ“šä¸Šè¨“ç·´æ¨¡å‹
    print("\nåœ¨å®Œæ•´è¨“ç·´é›†ä¸Šè¨“ç·´æœ€çµ‚æ¨¡å‹...")
    final_model, history = train_final_model(X_train, y_train, X_test, y_test, X_train.shape[1])
    
    # è¦–è¦ºåŒ–è¨“ç·´æ­·å²
    visualize_training_history(history)
    
    # è©•ä¼°æ¨¡å‹
    accuracy, roc_auc = evaluate_model(final_model, X_test, y_test)
    
    # è¨˜éŒ„æ¨¡å‹æ€§èƒ½
    log_roc_score(final_model, X_test, y_test)
    
    # ä¿å­˜æ¨¡å‹å’Œè½‰æ›å™¨ç”¨æ–¼éƒ¨ç½²
    # ç²å–æ‰€æœ‰ç‰¹å¾µåç¨±ï¼ˆåŒ…æ‹¬å·¥ç¨‹ç‰¹å¾µï¼‰
    all_features = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 
                   'thalach', 'exang', 'oldpeak', 'slope', 'thal', 
                   'age_thalach_ratio', 'bp_thalach_ratio', 'chol_age_ratio']
    
    save_deployment_model(final_model, scaler, selector, all_features)
    
    # ç¤ºç¯„å¦‚ä½•ä½¿ç”¨æ¨¡å‹é€²è¡Œé æ¸¬
    sample_patient = {
        'age': 63,
        'sex': 1,  # ç”·æ€§
        'cp': 3,   # å…¸å‹å¿ƒçµç—›
        'trestbps': 150,
        'chol': 233,
        'fbs': 1,
        'restecg': 0,
        'thalach': 180,
        'exang': 0,
        'oldpeak': 2.3,
        'slope': 0,
        'thal': 2
    }
    
    result = predict_heart_disease(sample_patient)
    print("\né æ¸¬çµæœç¤ºä¾‹:")
    print(f"é æ¸¬: {result['prediction']}")
    print(f"å¿ƒè‡Ÿç—…å¯èƒ½æ€§: {result['heart_disease_prob']:.2%}")
    
    print("\n======= æ¨¡å‹è¨“ç·´èˆ‡è©•ä¼°å®Œæˆ =======")

if __name__ == "__main__":
    # å°å…¥é¡å¤–éœ€è¦çš„åº«
    from sklearn.metrics import roc_auc_score
    main()